{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.under_sampling import NearMiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's load our datasets\n",
    "weather = pd.read_csv('../CapstoneTwo/weather_clean.csv')\n",
    "train = pd.read_csv('../CapstoneTwo/train_clean.csv')\n",
    "test = pd.read_csv('../CapstoneTwo/test_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's create functions to handle datetime\n",
    "def datetime(df):\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "#And extract the year, month, week, and day\n",
    "def ymwd(df):\n",
    "    df['Year'] = df['Date'].dt.year\n",
    "    df['Month'] = df['Date'].dt.month\n",
    "    df['Week'] = df['Date'].dt.isocalendar().week\n",
    "    df['Day'] = df['Date'].dt.day_of_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A function to shift weather features\n",
    "def lag_weather(df):\n",
    "\n",
    "    df['Tmax_1w'] = df['Tmax'].shift(7)\n",
    "    df['Tmax_2w'] = df['Tmax'].shift(14)\n",
    "    df['Tmax_3w'] = df['Tmax'].shift(21)\n",
    "\n",
    "    df['Tmin_1w'] = df['Tmin'].shift(7)\n",
    "    df['Tmin_2w'] = df['Tmin'].shift(14)\n",
    "    df['Tmin_3w'] = df['Tmin'].shift(21)\n",
    "\n",
    "    df['Tavg_1w'] = df['Tavg'].shift(7)\n",
    "    df['Tavg_2w'] = df['Tavg'].shift(14)\n",
    "    df['Tavg_3w'] = df['Tavg'].shift(21)\n",
    "\n",
    "    df['Depart_1w'] = df['Depart'].shift(7)\n",
    "    df['Depart_2w'] = df['Depart'].shift(14)\n",
    "    df['Depart_3w'] = df['Depart'].shift(21)\n",
    "\n",
    "    df['DewPoint_1w'] = df['DewPoint'].shift(7)\n",
    "    df['DewPoint_2w'] = df['DewPoint'].shift(14)\n",
    "    df['DewPoint_3w'] = df['DewPoint'].shift(21)\n",
    "\n",
    "    df['WetBulb_1w'] = df['WetBulb'].shift(7)\n",
    "    df['WetBulb_2w'] = df['WetBulb'].shift(14)\n",
    "    df['WetBulb_3w'] = df['WetBulb'].shift(21)\n",
    "\n",
    "    df['Heat_1w'] = df['Heat'].shift(7)\n",
    "    df['Heat_2w'] = df['Heat'].shift(14)\n",
    "    df['Heat_3w'] = df['Heat'].shift(21)\n",
    "\n",
    "    df['Cool_1w'] = df['Cool'].shift(7)\n",
    "    df['Cool_2w'] = df['Cool'].shift(14)\n",
    "    df['Cool_3w'] = df['Cool'].shift(21)\n",
    "\n",
    "    df['PrecipTotal_1w'] = df['PrecipTotal'].shift(7)\n",
    "    df['PrecipTotal_2w'] = df['PrecipTotal'].shift(14)\n",
    "    df['PrecipTotal_3w'] = df['PrecipTotal'].shift(21)\n",
    "\n",
    "    df['StnPressure_1w'] = df['StnPressure'].shift(7)\n",
    "    df['StnPressure_2w'] = df['StnPressure'].shift(14)\n",
    "    df['StnPressure_3w'] = df['StnPressure'].shift(21)\n",
    "\n",
    "    df['SeaLevel_1w'] = df['SeaLevel'].shift(7)\n",
    "    df['SeaLevel_2w'] = df['SeaLevel'].shift(14)\n",
    "    df['SeaLevel_3w'] = df['SeaLevel'].shift(21)\n",
    "\n",
    "    df['ResultSpeed_1w'] = df['ResultSpeed'].shift(7)\n",
    "    df['ResultSpeed_2w'] = df['ResultSpeed'].shift(14)\n",
    "    df['ResultSpeed_3w'] = df['ResultSpeed'].shift(21)\n",
    "\n",
    "    df['ResultDir_1w'] = df['ResultDir'].shift(7)\n",
    "    df['ResultDir_2w'] = df['ResultDir'].shift(14)\n",
    "    df['ResultDir_3w'] = df['ResultDir'].shift(21)\n",
    "\n",
    "    df['AvgSpeed_1w'] = df['AvgSpeed'].shift(7)\n",
    "    df['AvgSpeed_2w'] = df['AvgSpeed'].shift(14)\n",
    "    df['AvgSpeed_3w'] = df['AvgSpeed'].shift(21)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's create a function to get dummy variables for species\n",
    "def spec_dummies(df):\n",
    "    df = pd.concat([df, pd.get_dummies(df['Species'], drop_first=True)], axis=1)\n",
    "# We can drop the species column now that we have our dummies.\n",
    "    df.drop('Species', axis=1, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's create a function to cast features to float\n",
    "def float(df):\n",
    "    for col in df.columns:\n",
    "        try:\n",
    "            df[col] = df[col].astype(float)\n",
    "        except:\n",
    "            print(col, 'Cannot be transformed into a float')\n",
    "            pass\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We'll deal with date features in all of the datasets now\n",
    "datetime(weather)\n",
    "datetime(train)\n",
    "datetime(test)\n",
    "\n",
    "ymwd(weather)\n",
    "ymwd(train)\n",
    "ymwd(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's work on feature engineering for our weather dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's lag features in weather using our predefined function\n",
    "weather = lag_weather(weather)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We need to handle the missing values created from shifting features\n",
    "#Note: I need a better way to fill the NaN values\n",
    "weather.interpolate(method ='bfill', limit_direction ='backward', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's create dummy variables for species in our train and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = spec_dummies(train)\n",
    "test = spec_dummies(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combine datasets\n",
    "train_final = pd.merge(train, weather,on=['Date', 'Year', 'Month', 'Week', 'Day'],how='left')\n",
    "test_final = pd.merge(test, weather,on=['Date', 'Year', 'Month', 'Week', 'Day'],how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's save our combined datasets\n",
    "train_final.to_csv('../CapstoneTwo/train_final.csv',index=False)\n",
    "test_final.to_csv('../CapstoneTwo/test_final.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to get our X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove our target and use only numeric data\n",
    "X = train_final.drop(columns='WnvPresent')._get_numeric_data()\n",
    "#Isolate our target\n",
    "y = train_final['WnvPresent']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Call train_test_split with an 80/20 split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8404, 70)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scale our data (We chose to use MinMaxScaler for a light touch, and because the data isn't normally distributed.)\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "#Call scaler (do not fit on X_test)\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We'll address our imbalanced data with NearMiss to undersample\n",
    "nm = NearMiss(version=3)\n",
    "X_train_nm, y_train_nm = nm.fit_resample(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(624, 70)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We can see the data is now balanced\n",
    "X_train_nm.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is my first pass at this section. I had a few questions.\n",
    "\n",
    "1. After shifting, what is the best way to fill all of those NaNs? Currently, it's backfilled but I know this isn't adequate.\n",
    "\n",
    "2. I'm a little confused on getting the X and y and if I should use train_test_split. Because we are given the test set, should I just set X_train = my training data with the target feature dropped, y_train as the training data target feature, X_test as the test data, which leaves me confused as to what y_test might be since there is no \"WnvPresent\" in the test dataset?\n",
    "\n",
    "3. I'm torn between MinMaxScaler and Standard Scaler. I went with MinMaxScaler because our data does not seem normally distributed.\n",
    "\n",
    "4. Curious why we should undersample rather than oversample. The data is balanced, but seemingly leaves very few records in the training set. I chose NearMiss as it seemed pretty thorough from my research.\n",
    "\n",
    "5. Finally, I assume my order is correct - split the data, scale it (only transforming on X_test,) and finally undersampling."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2bf97991b2f0d81bc887135954a0bba4462368bc46a8d93544241e27e5301073"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
